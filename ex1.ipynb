{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5919459-041a-420d-b550-081c2bf1b19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebff4e22-d9df-4c27-98a1-525987e18ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/23 15:40:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/04/23 15:41:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/04/23 15:41:00 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".master(\"local\") \\\n",
    ".config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    ".config(\"spark.executor.memory\", \"500mb\") \\\n",
    ".appName(\"Exercise1\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76afe6b5-64ee-41a9-98ac-f1e412536680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales_table = spark.read.parquet('DatasetToCompleteTheSixSparkExercises/sales_parquet')\n",
    "products_table = spark.read.parquet('DatasetToCompleteTheSixSparkExercises/products_parquet')\n",
    "sellers_table = spark.read.parquet('DatasetToCompleteTheSixSparkExercises/sellers_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f4bb7e-f716-4ae0-91e5-697910fbfec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|order_id|product_id|seller_id|      date|num_pieces_sold|       bill_raw_text|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|       1|         0|        0|2020-07-10|             26|kyeibuumwlyhuwksx...|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+------------+-----+\n",
      "|product_id|product_name|price|\n",
      "+----------+------------+-----+\n",
      "|         0|   product_0|   22|\n",
      "+----------+------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_table.show(1)\n",
    "products_table.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "511e5b19-fbd6-4bf6-87f1-04d8ca561be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_products = sales_table.join(products_table, on='product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "035ef3c8-788d-4f2d-8ec7-ecef499a0c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+----------+---------------+--------------------+----------------+-----+\n",
      "|product_id|order_id|seller_id|      date|num_pieces_sold|       bill_raw_text|    product_name|price|\n",
      "+----------+--------+---------+----------+---------------+--------------------+----------------+-----+\n",
      "|  10005243|12478308|        6|2020-07-04|             98|qfvpgiscflyjxphcq...|product_10005243|   44|\n",
      "+----------+--------+---------+----------+---------------+--------------------+----------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales_products.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "291baac4-a864-47a8-81f4-ca96d83ff984",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_products_avg = sales_products.withColumn('revenue', (col('num_pieces_sold') * col('price')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df1f4908-d367-4d66-90f8-ce829450dea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(revenue)|\n",
      "+------------------+\n",
      "|1246.1338560822878|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales_products_avg.select(avg('revenue')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00b30733-6408-47b1-a351-90884774bc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=====================================================>  (18 + 1) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|avg((price * num_pieces_sold))|\n",
      "+------------------------------+\n",
      "|            1246.1338560822878|\n",
      "+------------------------------+\n",
      "\n",
      "None\n",
      "Ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1 - Check and select the skewed keys \n",
    "# In this case we are retrieving the top 100 keys: these will be the only salted keys.\n",
    "results = sales_table.groupby(sales_table[\"product_id\"]).count().sort(col(\"count\").desc()).limit(100).collect()\n",
    "# Step 2 - What we want to do is:\n",
    "#  a. Duplicate the entries that we have in the dimension table for the most common products, e.g.\n",
    "#       product_0 will become: product_0-1, product_0-2, product_0-3 and so on\n",
    "#  b. On the sales table, we are going to replace \"product_0\" with a random duplicate (e.g. some of them \n",
    "#     will be replaced with product_0-1, others with product_0-2, etc.)\n",
    "# Using the new \"salted\" key will unskew the join\n",
    "# Let's create a dataset to do the trick\n",
    "REPLICATION_FACTOR = 101\n",
    "l = []\n",
    "replicated_products = []\n",
    "for _r in results:\n",
    "    replicated_products.append(_r[\"product_id\"])\n",
    "    for _rep in range(0, REPLICATION_FACTOR):\n",
    "        l.append((_r[\"product_id\"], _rep))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "replicated_df = rdd.map(lambda x: Row(product_id=x[0], replication=int(x[1])))\n",
    "replicated_df = spark.createDataFrame(replicated_df)\n",
    "\n",
    "#   Step 3: Generate the salted key\n",
    "products_table = products_table.join(broadcast(replicated_df),\n",
    "                                     products_table[\"product_id\"] == replicated_df[\"product_id\"], \"left\"). \\\n",
    "    withColumn(\"salted_join_key\", when(replicated_df[\"replication\"].isNull(), products_table[\"product_id\"]).otherwise(\n",
    "    concat(replicated_df[\"product_id\"], lit(\"-\"), replicated_df[\"replication\"])))\n",
    "\n",
    "sales_table = sales_table.withColumn(\"salted_join_key\", when(sales_table[\"product_id\"].isin(replicated_products),\n",
    "                                                             concat(sales_table[\"product_id\"], lit(\"-\"),\n",
    "                                                                    round(rand() * (REPLICATION_FACTOR - 1), 0).cast(\n",
    "                                                                        IntegerType()))).otherwise(\n",
    "    sales_table[\"product_id\"]))\n",
    "\n",
    "#   Step 4: Finally let's do the join\n",
    "print(sales_table.join(products_table, sales_table[\"salted_join_key\"] == products_table[\"salted_join_key\"],\n",
    "                       \"inner\").\n",
    "      agg(avg(products_table[\"price\"] * sales_table[\"num_pieces_sold\"])).show())\n",
    "\n",
    "print(\"Ok\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
